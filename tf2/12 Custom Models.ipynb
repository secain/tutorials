{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "    !pip install -U tqdm\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"deep\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors and operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix\n",
    "tf.constant([[1., 2., 3.], [4., 5., 6.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=42>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scalar\n",
    "tf.constant(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[2., 3.],\n",
       "       [5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[2.],\n",
       "       [5.]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ...等于前面所有维度都取:\n",
    "t[..., 1, tf.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conflicting Types\n",
    "Tensorflow不会自动执行任何类型转换, 需使用tf.cast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:AddV2]\n",
      "cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a double tensor [Op:AddV2]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tf.constant(2.0) + tf.constant(40)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)\n",
    "\n",
    "try:\n",
    "    tf.constant(2.0) + tf.constant(40., dtype=tf.float64)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=42.0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = tf.constant(40., dtype=tf.float64)\n",
    "tf.constant(2.0) + tf.cast(t2, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables\n",
    "基本不需要使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 1., 42.,  3.],\n",
       "       [ 4.,  5.,  6.]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[0, 1].assign(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 1., 42.,  0.],\n",
       "       [ 4.,  5.,  1.]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[:, 2].assign([0., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[100.,  42.,   0.],\n",
       "       [  4.,   5., 200.]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.scatter_nd_update(indices=[[0, 0], [1, 2]],\n",
    "                    updates=[100., 200.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[4., 5., 6.],\n",
       "       [1., 2., 3.]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_delta = tf.IndexedSlices(values=[[1., 2., 3.], [4., 5., 6.]],\n",
    "                                indices=[1, 0])\n",
    "v.scatter_update(sparse_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 返回一个损失的张量，而不是返回实例的平均损失\n",
    "# 这样可以根据要求使用类别权重或样本权重\n",
    "def huber_fn(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss  = tf.abs(error) - 0.5\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x252 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 3.5))\n",
    "z = np.linspace(-4, 4, 200)\n",
    "plt.plot(z, huber_fn(0, z), \"b-\", linewidth=2, label=\"huber($z$)\")\n",
    "plt.plot(z, z**2 / 2, \"b:\", linewidth=1, label=r\"$\\frac{1}{2}z^2$\")\n",
    "plt.plot([-1, -1], [0, huber_fn(0., -1.)], \"r--\")\n",
    "plt.plot([1, 1], [0, huber_fn(0., 1.)], \"r--\")\n",
    "plt.gca().axhline(y=0, color='k')\n",
    "plt.gca().axvline(x=0, color='k')\n",
    "plt.axis([-4, 4, 0, 4])\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"$z$\")\n",
    "plt.legend(fontsize=14)\n",
    "plt.title(\"Huber loss\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.0443 - mae: 1.4660 - val_loss: 0.2862 - val_mae: 0.5866\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 833us/step - loss: 0.2379 - mae: 0.5407 - val_loss: 0.2382 - val_mae: 0.5281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbab00b7630>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(loss=huber_fn, optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving/Loading Models with Custom Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2054 - mae: 0.4982 - val_loss: 0.2209 - val_mae: 0.5050\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 777us/step - loss: 0.1999 - mae: 0.4900 - val_loss: 0.2127 - val_mae: 0.4986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fba9810d5f8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save(\"my_model_with_a_custom_loss.h5\")\n",
    "model = keras.models.load_model(\"my_model_with_a_custom_loss.h5\",\n",
    "                                custom_objects={\"huber_fn\": huber_fn})\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 当损失函数本身含有可调参数，如阈值时, load_model时也需要设置其参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_huber(threshold=1.0):\n",
    "    def huber_fn(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss  = threshold * tf.abs(error) - threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    return huber_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2318 - mae: 0.4979 - val_loss: 0.2540 - val_mae: 0.4907\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 771us/step - loss: 0.2309 - mae: 0.4960 - val_loss: 0.2372 - val_mae: 0.4879\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=create_huber(2.0), optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "\n",
    "model.save(\"my_model_with_a_custom_loss_threshold_2.h5\")\n",
    "model = keras.models.load_model(\"my_model_with_a_custom_loss_threshold_2.h5\",\n",
    "                                custom_objects={\"huber_fn\": create_huber(2.0)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 或者通过继承`keras.losses.Loss`类，实现其`get_config()`方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberLoss(keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss  = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2240 - mae: 0.4892 - val_loss: 0.2327 - val_mae: 0.4751\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 779us/step - loss: 0.2229 - mae: 0.4871 - val_loss: 0.2164 - val_mae: 0.4728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=HuberLoss(2.), optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "\n",
    "model.save(\"my_model_with_a_custom_loss_class.h5\")\n",
    "model = keras.models.load_model(\"my_model_with_a_custom_loss_class.h5\", # TODO: check PR #25956\n",
    "                               custom_objects={\"HuberLoss\": HuberLoss})\n",
    "\n",
    "model.loss.threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Activation Functions, Initializers, Regularizers, and Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 等同于`keras.activations.softplus()`或`tf.nn.softplus()`\n",
    "* 等同于`keras.initializers.glorot_normal()`\n",
    "* 等同于`keras.regularizers.l1(0.01)`\n",
    "* 等同于`keras.contraints.nonneg()`或`tf.nn.relu()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softplus(z): # return value is just tf.nn.softplus(z)\n",
    "    return tf.math.log(tf.exp(z) + 1.0)\n",
    "\n",
    "def my_glorot_initializer(shape, dtype=tf.float32):\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "\n",
    "def my_positive_weights(weights): # return value is just tf.nn.relu(weights)\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1, activation=my_softplus,\n",
    "                       kernel_regularizer=my_l1_regularizer,\n",
    "                       kernel_constraint=my_positive_weights,\n",
    "                       kernel_initializer=my_glorot_initializer),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 2.3829 - mae: 1.1635 - val_loss: 1.4154 - val_mae: 0.5607\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 752us/step - loss: 0.6299 - mae: 0.5410 - val_loss: 1.4399 - val_mae: 0.5137\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fba98183240>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model_with_many_custom_parts.h5\")\n",
    "model = keras.models.load_model(\n",
    "    \"my_model_with_many_custom_parts.h5\",\n",
    "    custom_objects={\n",
    "       \"my_l1_regularizer\": my_l1_regularizer,\n",
    "       \"my_positive_weights\": my_positive_weights,\n",
    "       \"my_glorot_initializer\": my_glorot_initializer,\n",
    "       \"my_softplus\": my_softplus,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 同损失函数一样, 当需要保存参数时需继承对应的父类\n",
    "* Note that you must implement the `call()` method for losses, layers (including activa‐tion functions), and models, or the `__call__()` method for regularizers, initializers, and constraints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyL1Regularizer(keras.regularizers.Regularizer):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "    def __call__(self, weights):\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "    def get_config(self):\n",
    "        return {\"factor\": self.factor}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: if you use the same function as the loss and a metric, you may be surprised to see different results. This is generally just due to floating point precision errors: even though the mathematical equations are equivalent, the operations are not run in the same order, which can lead to small differences. Moreover, when using sample weights, there's more than just precision errors:\n",
    "* the **loss** since the start of the epoch is the mean of all batch losses seen so far. Each batch loss is **the sum of the weighted instance losses divided by the _batch size_** (not the sum of weights, so the batch loss is _not_ the weighted mean of the losses).\n",
    "* the **metric** since the start of the epoch is equal to **the sum of weighted instance losses divided by sum of all weights** seen so far. In other words, it is the weighted mean of all the instance losses. Not the same thing.\n",
    "\n",
    "If you do the math, you will find that loss = metric * mean of sample weights (plus some floating point precision error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 605us/step - loss: 1.4306 - huber_fn: 1.4306\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 564us/step - loss: 0.2840 - huber_fn: 0.2840\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fba98b1a278>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=create_huber(2.0), optimizer=\"nadam\", metrics=[create_huber(2.0)])\n",
    "model.fit(X_train_scaled, y_train, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 995us/step - loss: 0.1156 - huber_fn: 0.2338\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 950us/step - loss: 0.1125 - huber_fn: 0.2282\n"
     ]
    }
   ],
   "source": [
    "sample_weight = np.random.rand(len(y_train))\n",
    "history = model.fit(X_train_scaled, y_train, epochs=2, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.11558229476213455, 0.11599889315680392)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history[\"loss\"][0], history.history[\"huber_fn\"][0] * sample_weight.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming metrics\n",
    "* 通常情况：For each batch during training, Keras will compute this metric and keep track of its mean since the beginning of the epoch.\n",
    "* 当metric值不等于每个batch的metric的均值时(如分类任务中的precision), 需要一个流式指标对象, 每个batch后更新真实的metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.8>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = keras.metrics.Precision()\n",
    "# batch 1\n",
    "precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch 2\n",
    "precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'true_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>,\n",
       " <tf.Variable 'false_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'true_positives:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n",
       " <tf.Variable 'false_positives:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 重置变量\n",
    "precision.reset_states()\n",
    "precision.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberMetric(keras.metrics.Mean):\n",
    "    def __init__(self, threshold=1.0, name='HuberMetric', dtype=None):\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(threshold)\n",
    "        super().__init__(name=name, dtype=dtype)\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        metric = self.huber_fn(y_true, y_pred)\n",
    "        super(HuberMetric, self).update_state(metric, sample_weight)\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.Huber(2.0), optimizer=\"nadam\", weighted_metrics=[HuberMetric(2.0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 713us/step - loss: 0.6479 - HuberMetric: 1.3055\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 693us/step - loss: 0.1260 - HuberMetric: 0.2491\n"
     ]
    }
   ],
   "source": [
    "sample_weight = np.random.rand(len(y_train))\n",
    "history = model.fit(X_train_scaled.astype(np.float32), y_train.astype(np.float32),\n",
    "                    epochs=2, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.35711368918418884, 0.3571138122580369)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history[\"loss\"][0], history.history[\"HuberMetric\"][0] * sample_weight.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.36787945, 1.        , 2.7182817 ], dtype=float32)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Layer with no weights\n",
    "exponential_layer = keras.layers.Lambda(lambda x: tf.exp(x))\n",
    "exponential_layer([-1., 0., 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 构造函数(`__init__()`)接受基本参数(如input_shape, trainable和name), 保存超参数.\n",
    "* The `build()` method’s role is to create the layer’s variables by calling the `add_weight()` method for each weight. \n",
    "* The `call()` method performs the desired operations.\n",
    "* The `compute_output_shape()` method simply returns the shape of this layer’s outputs.\n",
    "* The `get_config()` method is just like in the previous custom classes. Note that saving the activation function’s full configuration by calling `keras.activa tions.serialize()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)# a custom stateful layer (i.e., a layer with weights)\n",
    "class MyDense(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = keras.activations.get(activation)\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            name=\"kernel\", shape=[batch_input_shape[-1], self.units],\n",
    "            initializer=\"glorot_normal\")\n",
    "        self.bias = self.add_weight(\n",
    "            name=\"bias\", shape=[self.units], initializer=\"zeros\")\n",
    "        super().build(batch_input_shape) # must be at the end\n",
    "\n",
    "    def call(self, X):\n",
    "        return self.activation(X @ self.kernel + self.bias)\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"units\": self.units,\n",
    "                \"activation\": keras.activations.serialize(self.activation)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 967us/step - loss: 4.1268 - val_loss: 0.9472\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 712us/step - loss: 0.7086 - val_loss: 0.6219\n",
      "162/162 [==============================] - 0s 379us/step - loss: 0.5474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5473727583885193"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    MyDense(30, activation=\"relu\", input_shape=input_shape),\n",
    "    MyDense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model_with_a_custom_layer.h5\")\n",
    "model = keras.models.load_model(\"my_model_with_a_custom_layer.h5\",\n",
    "                                custom_objects={\"MyDense\": MyDense})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMultiLayer(keras.layers.Layer):\n",
    "    def call(self, X):\n",
    "        X1, X2 = X\n",
    "        return X1 + X2, X1 * X2\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        batch_input_shape1, batch_input_shape2 = batch_input_shape\n",
    "        return [batch_input_shape1, batch_input_shape2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs1 = keras.layers.Input(shape=[2])\n",
    "inputs2 = keras.layers.Input(shape=[2])\n",
    "outputs1, outputs2 = MyMultiLayer()((inputs1, inputs2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your layer needs to have **a different behavior during training and during testing** (e.g., if it uses Dropout or BatchNormalization layers), then you must add a training argument to the `call()` method and use this argument to decide what to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras.layers.GaussianNoise\n",
    "class AddGaussianNoise(keras.layers.Layer):\n",
    "    def __init__(self, stddev, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def call(self, X, training=None):\n",
    "        if training:\n",
    "            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n",
    "            return X + noise\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return batch_input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "X_new_scaled = X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(keras.layers.Layer):\n",
    "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(n_neurons, activation=\"elu\",\n",
    "                                          kernel_initializer=\"he_normal\")\n",
    "                       for _ in range(n_layers)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        return inputs + Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualRegressor(keras.models.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(30, activation=\"elu\",\n",
    "                                          kernel_initializer=\"he_normal\")\n",
    "        self.block1 = ResidualBlock(2, 30)\n",
    "        self.block2 = ResidualBlock(2, 30)\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = self.hidden1(inputs)\n",
    "        for _ in range(1 + 3):\n",
    "            Z = self.block1(Z)\n",
    "        Z = self.block2(Z)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "363/363 [==============================] - 2s 1ms/step - loss: 22.7478\n",
      "Epoch 2/5\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.2735\n",
      "Epoch 3/5\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.9791\n",
      "Epoch 4/5\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5909\n",
      "Epoch 5/5\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5546\n",
      "162/162 [==============================] - 0s 569us/step - loss: 0.6496\n"
     ]
    }
   ],
   "source": [
    "model = ResidualRegressor(1)\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = model.fit(X_train_scaled, y_train, epochs=5)\n",
    "score = model.evaluate(X_test_scaled, y_test)\n",
    "y_pred = model.predict(X_new_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn, dense_2_layer_call_and_return_conditional_losses, dense_3_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn, dense_2_layer_call_and_return_conditional_losses, dense_3_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_custom_model.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_custom_model.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.9387\n",
      "Epoch 2/5\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6846\n",
      "Epoch 3/5\n",
      "363/363 [==============================] - 0s 995us/step - loss: 0.6166\n",
      "Epoch 4/5\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4438\n",
      "Epoch 5/5\n",
      "363/363 [==============================] - 0s 984us/step - loss: 0.5044\n"
     ]
    }
   ],
   "source": [
    "model.save(\"my_custom_model.ckpt\")\n",
    "model = keras.models.load_model(\"my_custom_model.ckpt\")\n",
    "history = model.fit(X_train_scaled, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model using the sequential API instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "block1 = ResidualBlock(2, 30)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    block1, block1, block1, block1,\n",
    "    ResidualBlock(2, 30),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses and Metrics Based on Model Internals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The constructor creates the DNN with five dense hidden layers and one dense output layer.\n",
    "* The `build()` method creates an extra dense layer which will be used to recon‐struct the inputs of the model. It must be created here because its number of units must be equal to the number of inputs, and this number is unknown before the `build()` method is called.\n",
    "* The call() method processes the inputs through all five hidden layers, then passes the result through the reconstruction layer, which produces the recon‐struction.\n",
    "* Then the call() method computes the reconstruction loss (the mean squared difference between the reconstruction and the inputs), and adds it to the model’s list of losses using the add_loss() method.11 Notice that we scale down the reconstruction loss by multiplying it by 0.05 (this is a hyperparameter you can tune). This ensures that the reconstruction loss does not dominate the main loss.\n",
    "* Finally, the `call()` method passes the output of the hidden layers to the output layer and returns its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReconstructingRegressor(keras.models.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(30, activation=\"selu\",\n",
    "                                          kernel_initializer=\"lecun_normal\")\n",
    "                       for _ in range(5)]\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "        self.reconstruction_mean = keras.metrics.Mean(name=\"reconstruction_error\")\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        n_inputs = batch_input_shape[-1]\n",
    "        self.reconstruct = keras.layers.Dense(n_inputs)\n",
    "        super().build(batch_input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        reconstruction = self.reconstruct(Z)\n",
    "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
    "        self.add_loss(0.05 * recon_loss)\n",
    "        if training:\n",
    "            result = self.reconstruction_mean(recon_loss)\n",
    "            self.add_metric(result)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "InaccessibleTensorError",
     "evalue": "in user code:\n\n    /app/anaconda3/envs/studio/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /app/anaconda3/envs/studio/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /app/anaconda3/envs/studio/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /app/anaconda3/envs/studio/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /app/anaconda3/envs/studio/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /app/anaconda3/envs/studio/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /app/anaconda3/envs/studio/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:756 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /app/anaconda3/envs/studio/lib/python3.6/site-packages/tensorflow/python/keras/engine/compile_utils.py:229 __call__\n        reg_loss = math_ops.add_n(regularization_losses)\n    /app/anaconda3/envs/studio/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /app/anaconda3/envs/studio/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3572 add_n\n        return gen_math_ops.add_n(inputs, name=name)\n    /app/anaconda3/envs/studio/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py:419 add_n\n        \"AddN\", inputs=inputs, name=name)\n    /app/anaconda3/envs/studio/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:750 _apply_op_helper\n        attrs=attr_protos, op_def=op_def)\n    /app/anaconda3/envs/studio/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py:588 _create_op_internal\n        inp = self.capture(inp)\n    /app/anaconda3/envs/studio/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py:638 capture\n        % (tensor, tensor.graph, self))\n\n    InaccessibleTensorError: The tensor 'Tensor(\"mul:0\", shape=(), dtype=float32)' cannot be accessed here: it is defined in another function or code block. Use return values, explicit Python locals or TensorFlow collections to access it. Defined in: FuncGraph(name=build_graph, id=140439413508584); accessed from: FuncGraph(name=train_function, id=140439413552856).\n    \n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInaccessibleTensorError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-02c0c12e003a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReconstructingRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mse\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nadam\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/anaconda3/envs/studio/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/anaconda3/envs/studio/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/anaconda3/envs/studio/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/anaconda3/envs/studio/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 726\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/anaconda3/envs/studio/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/anaconda3/envs/studio/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/anaconda3/envs/studio/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/anaconda3/envs/studio/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/anaconda3/envs/studio/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/anaconda3/envs/studio/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInaccessibleTensorError\u001b[0m: in user code:\n\n    /app/anaconda3/envs/studio/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /app/anaconda3/envs/studio/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /app/anaconda3/envs/studio/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /app/anaconda3/envs/studio/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /app/anaconda3/envs/studio/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /app/anaconda3/envs/studio/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /app/anaconda3/envs/studio/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:756 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /app/anaconda3/envs/studio/lib/python3.6/site-packages/tensorflow/python/keras/engine/compile_utils.py:229 __call__\n        reg_loss = math_ops.add_n(regularization_losses)\n    /app/anaconda3/envs/studio/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /app/anaconda3/envs/studio/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3572 add_n\n        return gen_math_ops.add_n(inputs, name=name)\n    /app/anaconda3/envs/studio/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py:419 add_n\n        \"AddN\", inputs=inputs, name=name)\n    /app/anaconda3/envs/studio/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:750 _apply_op_helper\n        attrs=attr_protos, op_def=op_def)\n    /app/anaconda3/envs/studio/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py:588 _create_op_internal\n        inp = self.capture(inp)\n    /app/anaconda3/envs/studio/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py:638 capture\n        % (tensor, tensor.graph, self))\n\n    InaccessibleTensorError: The tensor 'Tensor(\"mul:0\", shape=(), dtype=float32)' cannot be accessed here: it is defined in another function or code block. Use return values, explicit Python locals or TensorFlow collections to access it. Defined in: FuncGraph(name=build_graph, id=140439413508584); accessed from: FuncGraph(name=train_function, id=140439413552856).\n    \n"
     ]
    }
   ],
   "source": [
    "model = ReconstructingRegressor(1)\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = model.fit(X_train_scaled, y_train, epochs=2)\n",
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Gradients with Autodiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + 2 * w1 * w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "gradients = tape.gradient(z, [w1, w2])\n",
    "gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases you may want to stop gradients from backpropagating through some part of your neural network. \n",
    "\n",
    "To do this, you must use the `tf.stop_gradient()` function returns its inputs during the forward pass (like `tf.identity()`), but it does not let gradients through during backpropagation (it acts like a constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=30.0>, None]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + tf.stop_gradient(2 * w1 * w2)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "tape.gradient(z, [w1, w2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Training Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_reg = keras.regularizers.l2(0.05)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=l2_reg),\n",
    "    keras.layers.Dense(1, kernel_regularizer=l2_reg)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(X, y, batch_size=32):\n",
    "    idx = np.random.randint(len(X), size=batch_size)\n",
    "    return X[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_status_bar(iteration, total, loss, metrics=None):\n",
    "    metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result())\n",
    "                         for m in [loss] + (metrics or [])])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{}/{} - \".format(iteration, total) + metrics,\n",
    "          end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 - loss: 0.0900 - mean_square: 858.5000\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "mean_loss = keras.metrics.Mean(name=\"loss\")\n",
    "mean_square = keras.metrics.Mean(name=\"mean_square\")\n",
    "for i in range(1, 50 + 1):\n",
    "    loss = 1 / i\n",
    "    mean_loss(loss)\n",
    "    mean_square(i ** 2)\n",
    "    print_status_bar(i, 50, mean_loss, [mean_square])\n",
    "    time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 3500/10000 [=>....]'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A fancier version with a progress bar:\n",
    "\n",
    "def progress_bar(iteration, total, size=30):\n",
    "    running = iteration < total\n",
    "    c = \">\" if running else \"=\"\n",
    "    p = (size - 1) * iteration // total\n",
    "    fmt = \"{{:-{}d}}/{{}} [{{}}]\".format(len(str(total)))\n",
    "    params = [iteration, total, \"=\" * p + c + \".\" * (size - p - 1)]\n",
    "    return fmt.format(*params)\n",
    "\n",
    "progress_bar(3500, 10000, size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_status_bar(iteration, total, loss, metrics=None, size=30):\n",
    "    metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result())\n",
    "                         for m in [loss] + (metrics or [])])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{} - {}\".format(progress_bar(iteration, total), metrics), end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - loss: 0.0900 - mean_square: 858.5000\n"
     ]
    }
   ],
   "source": [
    "mean_loss = keras.metrics.Mean(name=\"loss\")\n",
    "mean_square = keras.metrics.Mean(name=\"mean_square\")\n",
    "for i in range(1, 50 + 1):\n",
    "    loss = 1 / i\n",
    "    mean_loss(loss)\n",
    "    mean_square(i ** 2)\n",
    "    print_status_bar(i, 50, mean_loss, [mean_square])\n",
    "    time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "optimizer = keras.optimizers.Nadam(lr=0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "mean_loss = keras.metrics.Mean()\n",
    "metrics = [keras.metrics.MeanAbsoluteError()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "11610/11610 [==============================] - mean: 1.3955 - mean_absolute_error: 0.5722\n",
      "Epoch 2/5\n",
      "11610/11610 [==============================] - mean: 0.6774 - mean_absolute_error: 0.5280\n",
      "Epoch 3/5\n",
      "11610/11610 [==============================] - mean: 0.6351 - mean_absolute_error: 0.5177\n",
      "Epoch 4/5\n",
      "11610/11610 [==============================] - mean: 0.6384 - mean_absolute_error: 0.5181\n",
      "Epoch 5/5\n",
      "11610/11610 [==============================] - mean: 0.6440 - mean_absolute_error: 0.5222\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
    "    for step in range(1, n_steps + 1):\n",
    "        X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        for variable in model.variables:\n",
    "            if variable.constraint is not None:\n",
    "                variable.assign(variable.constraint(variable))\n",
    "        mean_loss(loss)\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "        print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)\n",
    "    print_status_bar(len(y_train), len(y_train), mean_loss, metrics)\n",
    "    for metric in [mean_loss] + metrics:\n",
    "        metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To run this cell, please install tqdm, ipywidgets and restart Jupyter\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from tqdm.notebook import trange\n",
    "    from collections import OrderedDict\n",
    "    with trange(1, n_epochs + 1, desc=\"All epochs\") as epochs:\n",
    "        for epoch in epochs:\n",
    "            with trange(1, n_steps + 1, desc=\"Epoch {}/{}\".format(epoch, n_epochs)) as steps:\n",
    "                for step in steps:\n",
    "                    X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
    "                    with tf.GradientTape() as tape:\n",
    "                        y_pred = model(X_batch)\n",
    "                        main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "                        loss = tf.add_n([main_loss] + model.losses)\n",
    "                    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "                    for variable in model.variables:\n",
    "                        if variable.constraint is not None:\n",
    "                            variable.assign(variable.constraint(variable))                    \n",
    "                    status = OrderedDict()\n",
    "                    mean_loss(loss)\n",
    "                    status[\"loss\"] = mean_loss.result().numpy()\n",
    "                    for metric in metrics:\n",
    "                        metric(y_batch, y_pred)\n",
    "                        status[metric.name] = metric.result().numpy()\n",
    "                    steps.set_postfix(status)\n",
    "            for metric in [mean_loss] + metrics:\n",
    "                metric.reset_states()\n",
    "except ImportError as ex:\n",
    "    print(\"To run this cell, please install tqdm, ipywidgets and restart Jupyter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time, converting a Python function that performs TensorFlow operations into a TF Function is trivial: decorate it with `@tf.function` or let Keras take care of it for you. However, there are a few rules to respect:\n",
    "\n",
    "* If you call any external library, including NumPy or even the standard library, this call will run only during tracing; it will not be part of the graph. Indeed, a TensorFlow graph can only include TensorFlow constructs (tensors, operations, variables, datasets, and so on). So, make sure you use `tf.reduce_sum()` instead of `np.sum()`, `tf.sort()` instead of the built-in `sorted()` function, and so on (unless you really want the code to run only during tracing). This has a few additional implications:\n",
    "    * If you define a TF Function f(x) that just returns np.random.rand(), a random number will only be generated when the function is traced, so f(tf.constant(2.)) and f(tf.constant(3.)) will return the same random number, but f(tf.constant([2., 3.])) will return a different one. If you replace np.random.rand() with tf.random.uniform([]), then a new random number will be generated upon every call, since the operation will be part of the graph.\n",
    "    * If your non-TensorFlow code has side effects (such as logging something or updating a Python counter), then you should not expect those side effects to occur every time you call the TF Function, as they will only occur when the function is traced.\n",
    "    * You can wrap arbitrary Python code in a `tf.py_function()` operation, but doing so will hinder performance, as TensorFlow will not be able to do any graph optimization on this code. It will also reduce portability, as the graph will only run on platforms where Python is available (and where the right libraries are installed).\n",
    "* You can call other Python functions or TF Functions, but they should follow the same rules, as TensorFlow will capture their operations in the computation graph. Note that these other functions do not need to be decorated with `@tf.function`.\n",
    "* If the function creates a TensorFlow variable (or any other stateful TensorFlow object, such as a dataset or a queue), it must do so upon the very first call, and only then, or else you will get an exception. It is usually preferable to create variables outside of the TF Function (e.g., in the `build()` method of a custom layer). If you want to assign a new value to the variable, make sure you call its assign() method, instead of using the = operator.\n",
    "* The source code of your Python function should be available to TensorFlow. If the source code is unavailable (for example, if you define your function in the Python shell, which does not give access to the source code, or if you deploy only the compiled *.pyc Python files to production), then the graph generation process will fail or have limited functionality.\n",
    "* TensorFlow will only capture for loops that iterate over a tensor or a dataset. So make sure you use for i in `tf.range(x)` rather than `for i in range(x)`, or else the loop will not be captured in the graph. Instead, it will run during tracing. (This may be what you want if the for loop is meant to build the graph, for example to create each layer in a neural network.)\n",
    "* As always, for performance reasons, you should prefer a vectorized implementation whenever you can, rather than using loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
